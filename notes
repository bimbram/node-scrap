Lesson 9. 01 - How to spoof add custom request headers


Lesson 8. The Browser Automation method
-What is browser automation
1. Controlling a browser with code instead of human live interaction can be named as browser automation

-PRO's & CON's
1. PRO: Easy to understand
2. PRO: Easy to write code
3. PRO: Less code than the request method
4. CON: limitations on the browser engine you use
5. CON: slower than Request method

-Examples
1. node-horseman
2. puppeteer (recommended!)
3. NightmareJs (based on Electron browser)

-When to use it?
1. When network resources/bandwidth don't matter
2. When website renders dynamically
3. When being super fast does not matter
4. When you want to build something fast and easy
5. When you don't want to deal that much with network inspection

-More info
1. You can disable the browser headless and you can actually inspect what happens with it live
2. You can simulate almost every action a human can do in a browser
3. You can screenshot pages / create pdf's
4. You can manipulate the dom, inject code and scrape!

Lesson 7. The Request library method
-What it is doing?
1. The simplest way of doing HTTP(s) requests
2. You can send GET, POST, PUT, DELETE, and more types of requests
3. Supports HTTP Basic Auth, Gzip, multipart-data post, custom headers, file streaming.. And a lot more

-Pro's & Con's
1. PRO - You control each request and every parameter
2. PRO - Very fast (compared to a headless browser)
3. CON - It can get messy if not done right
4. CON - You could potentially write more code (compared to a headless browser)

-When to use it?
1. Is API based
2. Has basic authentication
3. Is NOT dynamically rendered
4. Downloading files

Lesson 6. Running with Terminal CMD & Basics on VSCode Debugger
-Just a simple instruction how to debug in VSCode

Lesson 5. The biggest problem with scraping
-Maintenance & Stability
1. Everything work fine until it doesn't, it can work for one day then it can fail (because the website can change)

-How can you solve this?
1. You can't, you need to debug and fix it

Lesson 4. Why & When to Choose Scraping
-Why we want to scrape a website?
1. It can be more convenient
2. You could get more data out
3. You do not have an official api
4. You have a shitty official api

-When to scrape a website?
1. The website does not specifically say that it is not ok to scrape it
2. You get permission from the owner of the website / company

-Advice & Conclusion
1. Respect the ToS (terms of service) of the website that you want to scrape
2. Use the official API's where possible
3. Do not over do it, make sure to not bombard their website with request
4. Seek legal advice from a lawyer if you want to be 100% sure about a website, in case you want to be serious and you rely on that data

Lesson 3. Writing a Simple IMDB Scraper
-Finished creating the project
